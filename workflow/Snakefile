"""
Snakemake routine to orchestrate the requisite DDmisID stages:
    - PIDCalib2 setup
    - Spawning of PIDCalib2 jobs
    - Running of PIDCalib2 jobs -> efficiency histograms {!µ, reco partitions|!µ, µ}
    - Binned template creation for the species in the hadron-enriched makeup
    - Discretisation of the data into hadron-enriched, species-specific bins
    - Fit to the reco data bins with reco|!mu PID templates; account for cross-contamination between high-purity data partitions
    - Extraction of the true abundance of each !mu species
    - Weight assignment yielding the effective per-species count in the single-muon misID composition
"""

import shutil
import os
from snakemake.io import glob_wildcards    
import pyfiglet
from ddmisid.engine import config
from ddmisid.pid import PIDEffXJobFactory

# cleanup after completion
onsuccess: 
    shutil.rmtree(".snakemake/metadata")
    shutil.rmtree("bin") # cleanup pidcalib efficiencies
    shutil.rmtree("obs") # cleanup data partitions

rule all:
    """Target of the entire DDmisID workflow"""
    input:
        #"ddmisid.done"
        "pidcalib_efficiency_maps.done"

    run: 
        print(pyfiglet.figlet_format("misID", font = "isometric1" ))
        print(pyfiglet.figlet_format("done", font = "isometric1" ))


# ==================================================================================
#                          PID Efficiency Map Section
# ----------------------------------------------------------------------------------
#   - spawn the executables to run PIDCalib2 to generate the PID maps (!mu, mu)
#   - run the executables -> perf.pkl efficiecy histograms 
#   - post-process the histograms to handle negative/null efficiencies
#   - signal completion of all pideffx jobs (PIDCalib2 for particles, MC for ghosts)
# ==================================================================================
checkpoint generate_pid_efficiency_scripts:
    """
    Spawn pid-efficiency-extraction (pideffx) jobs as instructed in config YAML file. 
    Store the resulting bash scripts in a directory tree spawned within /bin/.
    """
    input:
        ".schema/validated_config.json"
    output:
        pid_efficiency_executable_parent_directory = directory("bin/"),
    run:
        # initialise the job script generator (PIDCalib2 for particles; bespoke MC processing for ghosts)
        job_factory = PIDEffXJobFactory()

        # generate the executable bash scripts reflecting the user-specified year, magnet polarity, pid category, and species
        job_factory.generate_jobs(output.pid_efficiency_executable_parent_directory)


rule extract_pid_effs:
    """
    Run the pideffx jobs as spawned in the previous step. Tallyt the resulting efficiency histograms.
    """
    input:
        bash_file = "bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/run.sh",
    output:
        pidcalib_hists = "bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf.pkl",
    log: 
        "logs/workflow/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/pideffx.log"
    run:
        shell("bash {input.bash_file} &> {log}")
 

rule process_pid_effs:
    """
    Within a nominal tolerance, round down efficiencies to 1e-6; this accounts also for the case where the efficiency is negative as computed by PIDCalib2
    """
    input:
        pidcalib_hists = "bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf.pkl",
    output:
        postprocessed_pid_hists = "bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf_postprocessed.pkl",
    params:
        executable = "ddmisid/snakemake/postprocess_pid.py"   
    log: 
        "logs/workflow/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/process_pideffx.log"
    run:
        shell("python {ddmisid/snakemake/postprocess} --input {input.pidcalib_hists} --output {output.postprocessed_pid_hists} &> {log}")


def aggregate_pid_effs(wildcards):
    '''
    Collect the variable number of sub-directories spawned by the PIDCalib2 setup, and post-processed efficiency histograms therein
    '''
    checkpoint_output = checkpoints.generate_pid_efficiency_scripts.get(**wildcards).output[0]
    year, magpol, dllmu_bin, species, eff_dirs = glob_wildcards(os.path.join(checkpoint_output, '{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/run.sh'))

    #return expand(checkpoint_output+"/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf_postprocessed.pkl", zip, year=year, magpol=magpol, dllmu_bin=dllmu_bin, species=species, eff_dirs=eff_dirs)
    return expand(checkpoint_output+"/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf.pkl", zip, year=year, magpol=magpol, dllmu_bin=dllmu_bin, species=species, eff_dirs=eff_dirs)


rule collect_pid_effs:
    """
    Signal completion of all pideffx jobs via dummy temporary file
    """
    input:
        aggregate_pid_effs
    output:
        combined = temp("pidcalib_efficiency_maps.done"),
    shell:
        "echo {input} > {output.combined}"


# # =========================================================================================
# #                                     Ghost Section
# # -----------------------------------------------------------------------------------------
# #   - Mimic PIDCalib2 on suitably truthmatched MC for ghost contamination modelling
# # =========================================================================================
# rule ghost_pid_effs_intermezzo:
#     """
#     Extract ghost-specific PID efficiencies from suitably truthmatched MC samples as proxy
#     No need to post-process, as ill-defined efficiencies in the ratio are logged as 0.0
#     """
#     input: 
#         "pidcalib_efficiency_maps.done"
#     output:
#         temp("all_pid_efficiency_maps.done"),
#     params:
#         year = config['pid']['years'],
#         magpol = config['pid']['magpols'],
#     shell:
#         "python ddmisid/ghost_pid_effs.py --year {params.year} --magpol {params.magpol} && touch {output}"


# # =========================================================================================
# #                     Hadron-Enriched Data Partitioning Section
# # -----------------------------------------------------------------------------------------
# #   - partition HE data into species-specific bins, within kinematic & occupancy partitions 
# # =========================================================================================
# checkpoint discretize_data:
#     """
#     Spawn the data bins in the form bespoke histograms (/obs/*/*.pkl)
#     These have all kinematic & occupancy cuts applied, as well has the PID cuts for the hadron-enriched bins specified in the config file
#     """
#     input:
#         data_path = config["data"]["input_path"] 
#     output:
#         directory("obs/"),
#     log: 
#        "logs/workflow/data_discretizer.log" 
#     shell:
#         "python ddmisid/data_discretizer.py {input.data_path} &> {log}"


# def aggregate_discretizer(wildcards):
#     """
#     Collect the variable number of sub-directories produced by partictioning the data
#     """
#     checkpoint_output = checkpoints.discretize_data.get(**wildcards).output[0]
#     p, eta, ntracks = glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/obs.pkl'))
#     return expand(checkpoint_output+"/{p}/{eta}/{ntracks}/obs.pkl", zip, p=p, eta=eta, ntracks=ntracks)


# rule collect_discretizer:
#     """
#     Signal completion of all data discretization jobs
#     """
#     input:
#         aggregate_discretizer
#     output:
#         combined = temp("data_partitions.done"),
#     shell:
#         "echo {input} > {output.combined}"


# # ================================================================================================
# #                               Binned Maximum Likelihood Section
# # ------------------------------------------------------------------------------------------------
# #   - build hadron-enriched templates for each species across all reco partitions
# #   - binned ML fit to the hadron-enriched data in bins of kinematics and occupancy
# #   - extract the true abundance of each species, accounting for cross-contamination 
# #   - build a histogram of the N_i/N_ref, per species, across all bins of kinematics and occupancy
# # ================================================================================================
# checkpoint make_templates:
#     input:
#         pidcalib_done = "all_pid_efficiency_maps.done", # NOTE: required to build the templates
#         data_partitions_done = "data_partitions.done" # avoid checkpoint conflict with data discretisation
#     params:
#         executable = "ddmisid/make_templates.py",
#         path_prefix = r"bin/2018/up/antimu_id", # TODO: retrieve from config, and deal with magpol
#     output: 
#         directory("templates/"),
#     shell:
#         "python {params.executable} {params.path_prefix}" 


# rule bml_fit: 
#     input:
#         # templates
#         pion_template = lambda wildcards: [
#             "templates/{p}/{eta}/{ntracks}/{species}.pkl".\
#             format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="pion")
#         ],
#         kaon_template = lambda wildcards: [
#             "templates/{p}/{eta}/{ntracks}/{species}.pkl".\
#             format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="kaon")
#         ],
#         proton_template = lambda wildcards: [
#             "templates/{p}/{eta}/{ntracks}/{species}.pkl".\
#             format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="proton")
#         ],
#         electron_template = lambda wildcards: [
#             "templates/{p}/{eta}/{ntracks}/{species}.pkl".\
#             format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="electron")
#         ],
#         ghost_template = lambda wildcards: [
#             "templates/{p}/{eta}/{ntracks}/{species}.pkl".\
#             format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="ghost")
#         ],
#         # observations
#         obs = lambda wildcards: [
#             "obs/{p}/{eta}/{ntracks}/obs.pkl".\
#             format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks) 
#         ],
#     params:
#         executable = "ddmisid/fit_reco.py"
#     log: 
#         "logs/workflow/{p}/{eta}/{ntracks}/bml_fit.log"
#     output:
#         # true abundance of each species, accounting for cross-contamination between reco bins
#         "postfit/{p}/{eta}/{ntracks}/yields.pkl",
#     run:
#         shell("python {params.executable}\
#             --pion {input.pion_template}\
#             --kaon {input.kaon_template}\
#             --proton {input.proton_template}\
#             --electron {input.electron_template}\
#             --ghost {input.ghost_template}\
#             --obs {input.obs} &> {log}" 
#         )


# def hadron_enriched_abundances(wildcards):
#     """Book the desired process outcome out of the checkpoint, so to be able to use wildcards to enact the fit"""
#     checkpoint_output = checkpoints.make_templates.get(**wildcards).output[0]
#     p = list(set(glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/{species}.pkl')).p))
#     eta = list(set(glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/{species}.pkl')).eta))
#     ntracks = list(set(glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/{species}.pkl')).ntracks))
    
#     return expand("postfit/{p}/{eta}/{ntracks}/yields.pkl", p=p, eta=eta, ntracks=ntracks)


# rule build_relative_abundance_lookup_table:
#     """Build a histogram of the N_i/N_ref, per species, across all bins of kinematics and occupancy"""
#     input:
#         hadron_enriched_abundances
#     params: 
#         executable = "ddmisid/collect_yields.py"
#     output:
#         "postfit/rel_abundances_hist.pkl", # global histogram; a lookup table in {species, kinematics, occupancy} -> N_i/N_ref
#     run:
#         shell("python {params.executable} --input {input} --output {output}")


# # =====================================================================================================================
# #                                       Extrapolation to Signal Region Section
# # ---------------------------------------------------------------------------------------------------------------------
# #   - Source the relative true abundance of each species in the control sample  
# #   - Extract misID weight through the linear combination of the above and eff(h->mu), unfolding eff(!isMuon & PIDmu<0)
# # =====================================================================================================================
# def fetch_pid_eff_sp_id(_dllmu_bin, _species, partition=None, _magpol="up"): # FIXME: expand to both magpol
#     def fetch_eff(wildcards):
#         '''
#         Collect the variable number of sub-directories produced by setup.py that live in /exec/,
#         and expand to use perfHists.root files produced by PIDCalib2
#         '''
#         checkpoint_output = checkpoints.generate_pid_efficiency_scripts.get(**wildcards).output[0]
        
#         year, eff_dirs = glob_wildcards(os.path.join(checkpoint_output, '{year}/'+_magpol+'/'+_dllmu_bin+'/'+_species+'/{eff_dirs}/run.sh'))
        
#         # care in handling the inclusive !muon species-specific efficiency histogram
#         match partition:
#             case None:
#                 return expand(checkpoint_output+"/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf_postprocessed.pkl", zip, year=year, magpol=_magpol, dllmu_bin=_dllmu_bin, species=_species, eff_dirs=eff_dirs)
#             case _: 
#                 return expand(checkpoint_output+"/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf_postprocessed.pkl", zip, year=year, magpol=_magpol, dllmu_bin=_dllmu_bin, species=_species, eff_dirs=partition) # user-defined `eff_dirs`
    
#     return fetch_eff


# # for magpol in ["up", "down"]: FIXME: need to expand to include magpol variety
# rule extract_misid_weights: 
#     """
#     In each bin of kinematics and occupancy, perform the weighted sum of the c_i * eff(h_i->mu)/eff(!isMuon & PIDmu<0), 
#     where c_i is the true abundance of each species in that bin of hadron-enriched control data.
#     That is, unfold the efficiency related to isolating the hadron-enriched control sample as a whole, and fold back in the 
#     species-specific misID efficiency driving the h->mu pollution in the signal channel.
#     """
#     input:
#         # $N_i/N_{\mathrm{ref}}$ for each species 
#         # ---------------------------------------
#         relative_true_abundances = "postfit/rel_abundances_hist.pkl",
#         # eff(h->mu) for each species
#         # -----------------------------
#         pion_to_mu_eff = fetch_pid_eff_sp_id("mu_id", "pion"),
#         electron_to_mu_eff = fetch_pid_eff_sp_id("mu_id", "electron"),
#         kaon_to_mu_eff = fetch_pid_eff_sp_id("mu_id", "kaon"),
#         proton_to_mu_eff = fetch_pid_eff_sp_id("mu_id", "proton"),
#         # FIXME: ghosts
#         # species-specific PID efficiencies for h \in hadron-enriched-dataset [no partitions required]
#         # --------------------------------------------------------------------------------------------
#         proton_to_antimu_eff = fetch_pid_eff_sp_id("antimu_id", "proton", "all"), # all signifies in {!isMuon & DLLmu<0}
#         kaon_to_antimu_eff = fetch_pid_eff_sp_id("antimu_id", "kaon", "all"),
#         pion_to_antimu_eff = fetch_pid_eff_sp_id("antimu_id", "pion", "all"),
#         electron_to_antimu_eff = fetch_pid_eff_sp_id("antimu_id", "electron", "all"),
#         # FIXME: global_antimuon_eff_ghosts = fetch_pid_eff_sp_id("antimu_id", "proton", "all"),
#         # hadron-enriched data, to which we want to assign weights
#         # --------------------------------------------------------
#         hadron_enriched_data = config["data"]["input_path"]
#     params:
#         executable = "ddmisid/assign_w_misid.py",
#         root_key = config['data']['root_config']['root_key'],
#         root_tree_name = config['data']['root_config']['root_tree_name'],
#         outfile_path = config['data']['output_path']
#     output:
#         temp("ddmisid.done") # signal completion
#     run:
#         shell("python {params.executable} --rel_abundances {input.relative_true_abundances} --obs {input.hadron_enriched_data}\
#         --proton_to_mu {input.proton_to_mu_eff} --pion_to_mu {input.pion_to_mu_eff} --kaon_to_mu {input.kaon_to_mu_eff} --electron_to_mu {input.electron_to_mu_eff}\
#         --proton_to_antimu {input.proton_to_antimu_eff} --pion_to_antimu {input.pion_to_antimu_eff} --kaon_to_antimu {input.kaon_to_antimu_eff} --electron_to_antimu {input.electron_to_antimu_eff}\
#         --output {params.outfile_path} --key {params.root_key} --tree {params.root_tree_name} \
#         && touch {output}") # temporary signal file only if main executable has run successfully