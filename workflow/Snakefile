"""
Snakemake routine to orchestrate the requisite DDmisID stages:
    - PIDCalib2 setup
    - Spawning of PIDCalib2 jobs
    - Running of PIDCalib2 jobs -> efficiency histograms {!µ, reco partitions|!µ, µ}
    - Binned template creation for the species in the hadron-enriched makeup
    - Discretisation of the data into hadron-enriched, species-specific bins
    - Fit to the reco data bins with reco|!mu PID templates; account for cross-contamination between high-purity data partitions
    - Extraction of the true abundance of each !mu species
    - Weight assignment yielding the effective per-species count in the single-muon misID composition
"""

import shutil
import os
from snakemake.io import glob_wildcards    
import pyfiglet
from ddmisid.engine import config
from ddmisid.pid import PIDEffXJobFactory, TemplateFactory
from ddmisid.utils import EfficiencyHistogramProcessor, load_hist
import pickle
from loguru import logger as rule_logger
from ddmisid.data_transforms import Discretiser
from ddmisid.utils import load_root, update_write_df, write_df, simple_load, extract_sel_dict_branches
from ddmisid.utils.binning import PIDCalibAliasFactory
import numpy as np

# cleanup after completion
onsuccess: 
    shutil.rmtree("workflow/snakemake/metadata")
    shutil.rmtree("workflow/snakemake/log")
    # shutil.rmtree("scratch/bin") # cleanup pidcalib efficiencies
    # shutil.rmtree("scratch/obs") # cleanup data partitions
    # shutil.rmtree("scratch/templates") # cleanup templates
    # shutil.rmtree("scratch/postfit") # cleanup postfit data

rule all:
    """Target of the entire DDmisID workflow"""
    input:
        "ddmisid.done"
    run: 
        print(pyfiglet.figlet_format("misID", font = "isometric1" ))
        print(pyfiglet.figlet_format("done", font = "isometric1" ))


# ==================================================================================
#                          PID Efficiency Map Section
# ----------------------------------------------------------------------------------
#   - spawn the executables to run PIDCalib2 to generate the PID maps (!mu, mu)
#   - run the executables -> perf.pkl efficiecy histograms 
#   - post-process the histograms to handle negative/null efficiencies
#   - signal completion of all pideffx jobs (PIDCalib2 for particles, MC for ghosts)
# ==================================================================================
checkpoint generate_pid_efficiency_scripts:
    """
    Spawn pid-efficiency-extraction (pideffx) jobs as instructed in config YAML file. 
    Store the resulting bash scripts in a directory tree spawned within /scratch/bin/.
    """
    input:
        ".schema/validated_config.json"
    output:
        pid_efficiency_executable_parent_directory = directory("scratch/bin/"),
    log:
        "logs/workflow/pideffx_script_generation.log"
    run:
        # init log
        rule_logger.add(str(log), level="INFO")

        try:
            rule_logger.info("Commencing pid-efficiency-extraction script generation")
            # initialise the job script generator (PIDCalib2 for particles; bespoke MC processing for ghosts)
            job_factory = PIDEffXJobFactory()

            # generate the executable bash scripts reflecting the user-specified year, magnet polarity, pid category, and species
            job_factory.generate_jobs(output.pid_efficiency_executable_parent_directory)

            # log completion
            rule_logger.info(f"PID efficiency extraction scripts successfully generated with directory head: {output.pid_efficiency_executable_parent_directory}")

        except Exception as e:
            # Capture any error and log it
            rule_logger.error(f"Error during PID efficiency script generation: {e}")
            raise e        


rule extract_pid_effs:
    """
    Run the pideffx jobs as spawned in the previous step. Tallyt the resulting efficiency histograms.
    """
    input:
        bash_file = "scratch/bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/run.sh",
    output:
        pidcalib_hists = "scratch/bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf.pkl",
    log: 
        "logs/workflow/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/pideffx.log"
    run:
        shell("bash {input.bash_file} &> {log}")
 

rule process_pid_effs:
    """
    Processing stage to handle negative/null PID efficiencies according to ddmisid.utils.histogram.EfficiencyHistogramProcessor()
    """
    input:
        pideffh = "scratch/bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/perf.pkl",
    output:
        postprocessed_pideffh = "scratch/bin/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/processed_perf.pkl",
    log: 
        "logs/workflow/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/process_pideffx.log"
    run:
        # init log
        rule_logger.add(str(log), level="INFO")

        try:
            rule_logger.info("Starting PID efficiency processing for {input.pideffh}")
            
            # Load and process the histogram
            process_pid_eff_hist = EfficiencyHistogramProcessor().process(load_hist(input.pideffh))
            
            # Write the processed histogram to the output
            with open(output.postprocessed_pideffh, "wb") as f:
                pickle.dump(process_pid_eff_hist, f)

            rule_logger.info(f"Successfully processed PID efficiency histogram. Output saved to {output.postprocessed_pideffh}")

        except Exception as e:
            # Capture any error and log it
            rule_logger.error(f"Error during PID efficiency processing: {e}")
            raise e


def aggregate_pid_effs(wildcards):
    '''
    Collect the variable number of sub-directories spawned by PIDEffX(), and post-processed efficiency histograms therein.
    '''
    checkpoint_output = checkpoints.generate_pid_efficiency_scripts.get(**wildcards).output[0]
    year, magpol, dllmu_bin, species, eff_dirs = glob_wildcards(os.path.join(checkpoint_output, '{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/run.sh'))

    return expand(checkpoint_output+"/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/processed_perf.pkl", zip, year=year, magpol=magpol, dllmu_bin=dllmu_bin, species=species, eff_dirs=eff_dirs)


rule collect_pid_effs:
    """
    Signal completion of all pideffx jobs via dummy temporary file
    """
    input:
        aggregate_pid_effs
    output:
        combined = temp("pideffx.done"),
    shell:
        "echo {input} > {output.combined}"


# =========================================================================================
#                     Observational Data Partitioning Section
# -----------------------------------------------------------------------------------------
#   - partition the control data into into species-specific bin, per kinematic & occupancy  
# =========================================================================================
checkpoint discretize_data:
    """
    Spawn the data bins in the form of histograms in the directory tree obs/.../*.pkl.
    These have all kinematic & occupancy cuts applied, as well has the PID cuts for the hadron-enriched bins specified in the config file.
    """
    input:
        data = config.data.input_path,
    output:
        directory("scratch/obs/"),
    log: 
       "logs/workflow/data_discretizer.log" 
    run:
        # init log
        rule_logger.add(str(log), level="INFO")
        
        # build a data binning with the bespoke, coarser binning `sweight binning` to ensure sufficient stats in the binned ML fits
        data_binning = {}
        pidcalib_alias_factory = PIDCalibAliasFactory()
        for key, prefix in config.data.data_prefixes.items():
            data_branch_prefix = f"{prefix}_" if prefix else ""
            data_binning[f"{data_branch_prefix}{key}"] = config.pid.sweight_binning[
                pidcalib_alias_factory.process_variable(var=key, year=config.pid.year) 
            ]

        # load data, reading] in only BOI, as will fill obs hist only
        control_dataset = load_root(
            path=input.data,#.input_path.2018.up,
            key=config.data.data_key,
            tree=config.data.data_tree,
            max_events=None,
            library="pd",
            branches=list(
                set(  # avoid duplication of branches
                    extract_sel_dict_branches(config.data.data_reco_partitions) + list(data_binning.keys())
                )
            ),  # extract the inclusive set of branches used in any selection definig the hadron-enriched partitions + binning variables
        )

        # discretise the dataset into reco categories
        d = Discretiser(
            binning=data_binning,
            reco_cuts=config.data.data_reco_partitions, 
            data=control_dataset,
            reco_label="reco",
        )
        reco_h = d.discretize()

        # save the histograms, looping through bins, projecting out the reco category in each bin
        bin_indices = [range(len(axis)) for axis in reco_h.axes[:-1]]  # Ignore the last axis (reco category)
        
        # Iterate over all combinations of bin indices for the variable number of axes
        for bin_combination in np.ndindex(*[len(axis) for axis in reco_h.axes[:-1]]):

            # Generate bin labels dynamically based on the axes names and bin values
            bin_labels = []
            for axis, index in zip(reco_h.axes[:-1], bin_combination):
                bin_range = axis[index]
                bin_name = axis.name
                bin_label = f"{bin_range[0]}-{bin_range[1]}" if isinstance(bin_range[0], (int, float)) else f"{bin_range[0]}-{bin_range[1]}"
                bin_labels.append(f"{bin_label}")

            # Generate a directory path from the bin labels
            bin_path = "/".join(bin_labels)

            # Save the histogram for the current bin combination, projecting out the reco category
            d.save_histogram(
                hist=reco_h[*bin_combination,...],
                path=f"scratch/obs/{bin_path}/obs.pkl",
            )


def aggregate_discretizer(wildcards):
    """
    Collect the variable number of sub-directories produced by partictioning the data
    """
    checkpoint_output = checkpoints.discretize_data.get(**wildcards).output[0]
    p, eta, ntracks = glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/obs.pkl'))
    return expand(checkpoint_output+"/{p}/{eta}/{ntracks}/obs.pkl", zip, p=p, eta=eta, ntracks=ntracks)


rule collect_discretizer:
    """
    Signal completion of all data discretization jobs
    """
    input:
        aggregate_discretizer
    output:
        combined = temp("data_partitions.done"),
    shell:
        "echo {input} > {output.combined}"


# # ================================================================================================
# #                               Binned Maximum Likelihood Section
# # ------------------------------------------------------------------------------------------------
# #   - build hadron-enriched templates for each species across all reco partitions
# #   - binned ML fit to the hadron-enriched data in bins of kinematics and occupancy
# #   - extract the true abundance of each species, accounting for cross-contamination 
# #   - build a histogram of the N_i/N_ref, per species, across all bins of kinematics and occupancy
# # ================================================================================================
checkpoint make_templates:
    input:
        # coalescence of pideffx and data discretisation
        pidcalib_done = "pideffx.done",
        data_partitions_done = "data_partitions.done" 
    params:
        path_prefix = r"scratch/bin/2018/up/control", # FIXME: retrieve from config, and deal with magpol
    output: 
        dir = directory("scratch/templates/"), 
    run:
        # # init log

        # initialise the template factory
        factory = TemplateFactory()


        # build the templates for each species across all reco partitions
        try:
            for species in [partition[:-len("_like")] if partition.endswith("_like") else partition for partition in config.pid.reco_partitions]:

                # register species-specific kind of template [default: `BinnedTemplate`]
                factory.register_species(species) 

                # fetch the relevant histograms accross all reco partitions
                species_template = factory.fetch_template_maker(
                        species, str(params.path_prefix), config.pid.reco_partitions
                )

                # build the binned template
                species_template.make_hist()

                # save the histogram in a directory tree spawned from `scratch/templates`
                species_template.save_hist(config.pid.sweight_binning, "scratch/templates")

        except Exception as e:
            # Capture any error and log it
            #rule_logger.error(f"Error during template creation: {e}")
            raise e


rule bml_fit: 
    input:
        # templates
        pion_template = lambda wildcards: [
            "scratch/templates/{p}/{eta}/{ntracks}/{species}.pkl".\
            format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="pion")
        ],
        # kaon_template = lambda wildcards: [
        #     "scratch/templates/{p}/{eta}/{ntracks}/{species}.pkl".\
        #     format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="kaon")
        # ],
        muon_template = lambda wildcards: [
            "scratch/templates/{p}/{eta}/{ntracks}/{species}.pkl".\
            format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="muon")
        ],
        proton_template = lambda wildcards: [
            "scratch/templates/{p}/{eta}/{ntracks}/{species}.pkl".\
            format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="proton")
        ],
        electron_template = lambda wildcards: [
            "scratch/templates/{p}/{eta}/{ntracks}/{species}.pkl".\
            format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="electron")
        ],
        # ghost_template = lambda wildcards: [
        #     "scratch/templates/{p}/{eta}/{ntracks}/{species}.pkl".\
        #     format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks, species="ghost")
        # ],
        # observations
        obs = lambda wildcards: [
            "scratch/obs/{p}/{eta}/{ntracks}/obs.pkl".\
            format(p=wildcards.p, eta=wildcards.eta, ntracks=wildcards.ntracks) 
        ],
    params:
        executable = "ddmisid/snakemake/fit_reco.py"
    log: 
        "logs/workflow/{p}/{eta}/{ntracks}/bml_fit.log"
    output:
        # true abundance of each species, accounting for cross-contamination between reco bins
        "scratch/postfit/{p}/{eta}/{ntracks}/yields.pkl",
    run:
        shell("python {params.executable}\
            --pion {input.pion_template}\
            --muon {input.muon_template}\
            --proton {input.proton_template}\
            --electron {input.electron_template}\
            --obs {input.obs} &> {log}" 
        )

def control_species_abundances(wildcards):
    """Book the desired process outcome out of the checkpoint, so to be able to use wildcards to enact the fit"""
    checkpoint_output = checkpoints.make_templates.get(**wildcards).output[0]
    p = list(set(glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/{species}.pkl')).p))
    eta = list(set(glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/{species}.pkl')).eta))
    ntracks = list(set(glob_wildcards(os.path.join(checkpoint_output, '{p}/{eta}/{ntracks}/{species}.pkl')).ntracks))
    
    return expand("scratch/postfit/{p}/{eta}/{ntracks}/yields.pkl", p=p, eta=eta, ntracks=ntracks)


rule build_relative_abundance_lookup_table:
    """Build a histogram of the N_i/N_ref, per species, across all bins of kinematics and occupancy"""
    input:
        control_species_abundances
    params: 
        executable = "ddmisid/snakemake/collect_yields.py"
    output:
        "scratch/postfit/rel_abundances_hist.pkl", # global histogram; a lookup table in {species, kinematics, occupancy} -> N_i/N_ref
    run:
        shell("python {params.executable} --input {input} --output {output}")


# =====================================================================================================================
#                                       Extrapolation to Signal Region Section
# ---------------------------------------------------------------------------------------------------------------------
#   - Source the relative true abundance of each species in the control sample  
#   - Extract misID weight through the linear combination of the above and eff(h->mu), unfolding eff(!isMuon & PIDmu<0)
# =====================================================================================================================
def fetch_pid_eff_sp_id(_pid_region, _species, partition=None): # FIXME: expand to both magpol
    def fetch_eff(wildcards):
        '''
        Collect the PID maps generated upstream in the workflow
        '''
        checkpoint_output = checkpoints.generate_pid_efficiency_scripts.get(**wildcards).output[0]
        year, magpol, eff_dirs = glob_wildcards(os.path.join(checkpoint_output, '{year}/{magpol}/'+_pid_region+'/'+_species+'/{eff_dirs}/run.sh'))

        # care in handling the inclusive !muon species-specific efficiency histogram
        match partition:
            case None:
                return expand(checkpoint_output+"/{year}/{magpol}/{pid_region}/{species}/{eff_dirs}/processed_perf.pkl", zip, year=year, magpol=magpol, pid_region=_pid_region, species=_species, eff_dirs=eff_dirs)
            case _: 
                return expand(checkpoint_output+"/{year}/{magpol}/{pid_region}/{species}/{eff_dirs}/processed_perf.pkl", zip, year=year, magpol=magpol, pid_region=_pid_region, species=_species, eff_dirs=partition) # user-defined `eff_dirs`
    
    return fetch_eff

# def aggregate_pid_effs(wildcards):
#     '''
#     Collect the variable number of sub-directories spawned by PIDEffX(), and post-processed efficiency histograms therein.
#     '''
#     checkpoint_output = checkpoints.generate_pid_efficiency_scripts.get(**wildcards).output[0]
#     year, magpol, dllmu_bin, species, eff_dirs = glob_wildcards(os.path.join(checkpoint_output, '{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/run.sh'))

#     return expand(checkpoint_output+"/{year}/{magpol}/{dllmu_bin}/{species}/{eff_dirs}/processed_perf.pkl", zip, year=year, magpol=magpol, dllmu_bin=dllmu_bin, species=species, eff_dirs=eff_dirs)



# for magpol in ["up", "down"]: FIXME: need to expand to include magpol variety
rule extract_misid_weights: 
    """
    In each bin of kinematics and occupancy, perform the weighted sum of the c_i * eff(h_i->mu)/eff(!isMuon & PIDmu<0), 
    where c_i is the true abundance of each species in that bin of hadron-enriched control data.
    That is, unfold the efficiency related to isolating the hadron-enriched control sample as a whole, and fold back in the 
    species-specific misID efficiency driving the h->mu pollution in the signal channel.
    """
    input:
        # $N_i/N_{\mathrm{ref}}$ for each species 
        # ---------------------------------------
        relative_true_abundances = "scratch/postfit/rel_abundances_hist.pkl",
        # eff(h->mu) for each species
        # -----------------------------
        pion_to_mu_eff = fetch_pid_eff_sp_id("target", "pion"),
        electron_to_mu_eff = fetch_pid_eff_sp_id("target", "electron"),
        #kaon_to_mu_eff = fetch_pid_eff_sp_id("target", "kaon"),
        muon_to_mu_eff = fetch_pid_eff_sp_id("target", "muon"),
        proton_to_mu_eff = fetch_pid_eff_sp_id("target", "proton"),
        # FIXME: ghosts missing
        # species-specific PID efficiencies for h \in hadron-enriched-dataset [no partitions required]
        # --------------------------------------------------------------------------------------------
        proton_to_antimu_eff = fetch_pid_eff_sp_id("control", "proton", "proton_to_control_like"), # all signifies in {!isMuon & DLLmu<0}
        #kaon_to_antimu_eff = fetch_pid_eff_sp_id("control", "kaon", "kaon_to_control_like"),
        muon_to_antimu_eff = fetch_pid_eff_sp_id("control", "muon", "muon_to_control_like"),
        pion_to_antimu_eff = fetch_pid_eff_sp_id("control", "pion", "pion_to_control_like"),
        electron_to_antimu_eff = fetch_pid_eff_sp_id("control", "electron", "electron_to_control_like"),
        # FIXME: global_antimuon_eff_ghosts = fetch_pid_eff_sp_id("control", "ghost", "control"),
        # hadron-enriched data, to which we want to assign weights
        # --------------------------------------------------------
        hadron_enriched_data = config.data.input_path
    params:
        executable = "ddmisid/snakemake/assign_w_misid.py",
        root_key = config.data.data_key,
        root_tree_name = config.data.data_tree,
        outfile_path = config.data.output_path
    output:
        temp("ddmisid.done") # signal completion
    run:
        shell("python {params.executable} --rel_abundances {input.relative_true_abundances} --obs {input.hadron_enriched_data}\
        --proton_to_mu {input.proton_to_mu_eff} --pion_to_mu {input.pion_to_mu_eff} --muon_to_mu {input.muon_to_mu_eff} --electron_to_mu {input.electron_to_mu_eff}\
        --proton_to_antimu {input.proton_to_antimu_eff} --pion_to_antimu {input.pion_to_antimu_eff} --muon_to_antimu {input.muon_to_antimu_eff} --electron_to_antimu {input.electron_to_antimu_eff}\
        --output {params.outfile_path} --key {params.root_key} --tree {params.root_tree_name} \
        && touch {output}") # temporary signal file only if main executable has run successfully